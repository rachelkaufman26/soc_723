---
title: "TE_hw_ch18"
author: "Rachel Kaufman"
date: "2023-03-31"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(fixest)
library(modelsummary)
library(tidyverse)
```

Homework for Chapter 18: Difference-in-Differences

##### **Question 1**
In the Event Studies chapter we estimated the effect of something that occurs at a specific time by just comparing before-event to after-event, without really using a control group. What assumption is made by no-control-group event studies that we don’t have to make with difference-in-differences?
In no-control-group event studies, we have to assume that we can close the back door through time (from treatment –> after event <– time –> outcome). Luckily with differences in differences (idk how much of this is luck but),the comparison group effectively controls for time because now there is a baseline comparison across time.

##### **Question 2**
Which of the following potential back doors is controlled for by comparing the treated group to a control group?
**b. There may be events affecting everyone that would change the outcome from before-treatment to after-treatment anyway**
By subtracting out the within variation for our control group, we can say that whatever is left is a result of the treatment for our treated group

##### **Question 3**
Consider a treatment and control group. Looking only at the pre-treatment period, they have exactly the same outcomes (zero gap between them in each period).

**3a.** Despite having exactly the same outcomes pre-treatment, it happens to be the case that parallel trends is violated for these two groups. How is this possible? Explain what it means for parallel trends to be violated in this case, or give an example of how it could be violated.
So if these two groups have the same outcomes pre-treatment this would go beyond any surface level validity concern. *that actually leads me to ask-- are we able to detect when a violation occurs in this instance if the slopes appear parallel? No, right?* An ex. of this could be that an event caused a drastic change in the trajectory/slope of one group’s line, and not the other, important to consider bc an outcome pre-treatment does not say anything about our slopes! In a similar sentiment if there was an instance where more data is gathered that shows different trajectories during the pretreatment period then the control and treatment groups would not have changed overtime in similar ways to begin with! Big violation right there.  

**3b.** If we estimate the causal effect in this case using difference-in-differences, even though parallel trends is violated, how much would our effect be off by? (note you won’t be able to give a specific number)
I mean, it could be off by like a fuck ton because the basis of deciding these differences depends on this. It would depended on the direction that the event caused the line to trend, but it would definitely cause either overestimating or underestimating of the treatment effect.

##### **Question 4**
Consider the below graph showing the average outcome for treated and control groups in the leadup to treatment (indicated by the dashed line), and also after treatment.

**4a.** Based on the prior trend, does it seem likely that parallel trends holds in this instance?
No def not... based on the prior trend the parallel trends assumption likely would not gold because teh distance btwn the two groups are not at all constant leading up to treatment, would be a bad comparison group for DiD. 

**4b.** If we estimate difference-in-differences anyway, are we likely to overestimate the actual causal effect, underestimate it, or get it right on average?
This would lead to underestimating the actual causal effect, if we had a line with a constant distance it wouldn't be soooo negative in its effect (as steve said in class, the intercept is more than allowed to be different!) 

##### **Question 5**
From March through May 2020, US and Canada COVID case rates followed similar trends (US rates were higher, but the trends were similar). You want to look at the effect of COVID restrictions enacted in Canada in late May 2020 on case rates. Is DID, with the US as a control group, a good way to estimate this effect? If not, what concerns would you have about this research design?
I mean yeah difference in differences would be fine for this because having similar trends holds the main assumption you should be confident on before ya know, actually using this for your model. BUT this def does depend on your estimand. If you split up the populations, or do anything with the sample, look at only women, a particular work sector, racial differences, etc., then this assumption may not continue to hold. Also, i would look into the role of the restrictions bc even following WHO guidelines doesn't mean much for enforcement and ya know we do not have a strong central government. Unsure how I could account for this off the top of my head though. 

##### **Question 6**
Consider the below table of mean outcomes, and calculate the difference-in-difference effect of treatment. Write out the equation you used to calculate it (i.e. show how the four numbers in the table are combined to get the estimate)
We can estimate the DID effect of treatment using the formula below: (TREATED post-treatment value - TREATED pre-treatment value) - (UNTREATED post-treatment value - UNTREATED pre-treatment value). If we plug in the values from our table, we get:
```{r}
(9-5)-(7.5-6)
```
Boom DiD is 2.5

##### **Question 7** 
You are planning to estimate whether voter-protection laws increase voter turnout. You note that, in 2015, a lot of new voter-protection laws were enacted in some provinces but not in others. Conveniently, no new laws were enacted in 2012, 2014, or 2016, so you decide to use 2012 and 2014 as your “before” periods and 2016 as “after”.
**7a.** Which of the following best describes what you’d want to regress state-and-year level “voter turnout” measures on?
**iv. A set of fixed effects for state, and for year, and an interaction between “is 2016” and “is a treated state”.**

**7b.** Unless you chose the final option in the previous question, specify which coefficient in that regression would give you the DID estimate.
The coefficient for the interaction term would give you the DID estimate.

##### **Question 8**
You are looking at a difference-in-difference design to estimate the effect of providing laptops to school children on their test scores. Look at the below regression output, in which “Treated” is an indicator that the school received laptops in 2008 as part of a new program (the untreated group did not receive any laptops until years after the sample window for this study ended), and “After” is an indicator for being after the year 2008.
```{r}
5.034/.993
```
Our T stat is 5.06. 
Assuming that parallel trends holds, the effect of laptops on test scores was 5.034, and this effect was statistically significant at the 95% level.


##### **Question 9**
A standard “prior trends” test might estimate a regression using the model Y= β_0+β_1 t+β_2 Treated+β_3 t×Treated+ε (only using data from before-treatment), where t is a time variable, Treated is an indicator for being in the treated group, and Y is an outcome variable, and look for a large/significant estimate of β_3. Explain why this test is performed, and specifically what it shows.
Test of prior trends is performed in order to compare the trends for the treated and untreated groups in the lead-up to the treatment period. The test will show us how different the trends are. If we find that B3 is unlikely to be 0, then the trends are different, which means we do get to see if our parallel assumptions hold.

##### **Question 10**
Consider the below graph with estimates from a dynamic difference-in-differences model for a treatment that occurs between periods 4 and 5, with 95% confidence intervals shown.
**10a.** What about this graph might make us concerned about our identification assumptions?
The confidence interval in period 1 is way above 0. The effect also seems to be pretty inconsistent across the three pre-treatment periods, so that makes us less confident that the post-treatment change was actually due to the treatment.

**10b.** Ignoring any concerns we have, what would we say is the effect of treatment on Y in this case? (note the height of the line in period 5 is about 3, in period 6 is about 1, and in period 7 is about .5).
We would say that there is an effect and it declines over time.

##### **Question 11**
Chapter 18.2.5 points out a problem with two-way fixed effects in cases where treatment is not all assigned at the same time, but rather different groups get treated at different times (a “rollout” design). In these designs, two-way fixed effects treats “already-treated” units, who were treated in earlier periods, as “control” units, as though they hadn’t gotten treated at all. However, there’s nothing theoretically wrong about using an already-treated unit as a control; the DID assumptions don’t require that the control group be untreated, just that the gap between treated and control doesn’t change when the treated group’s treatment goes into effect. Why are we so concerned, then, about using an already-treated group as a control? You can answer generally, or use as an example a DID with only two groups – an already-treated group and a newly-treated group. (hint: to do the example, try assuming the treatment only has an effect for the single period after treatment, and the already-treated group is treated exactly one period before the treated group)
We have to be concerned in the case of dynamic effects- effects can change over time, or be delayed, so this would mess with the slope and violate parallel trends. Another scenario that should concern us is if the treatment effect varies across groups, as that would also violate parallel trends.

### **Coding Questions**
##### **Question 1**
```{r}
library(broom)
library(lubridate)
library(readr)
sourdough_trends <- read_csv("Documents/soc_723/sourdough_trends.csv")
View(sourdough_trends)
```

Putting it into a DF
```{r}
sr <- as.data.frame(sourdough_trends)

sr <- sr %>% 
  select(date, hits, keyword)

sr <- sr %>% 
  mutate(date = as.Date(date))

glimpse(sr)
```

##### **Question 2**
Make a line graph with date on the x-axis and hits on the y-axis, with a separate line for each keyword. Also add a vertical line for the “start of the pandemic” which we’ll decide for our purposes is March 15, 2020.

```{r}
#make 'eventdate' variable
eventdate <- as.Date('2020-03-15')

#make a line graph
ggplot(data = sr, 
  mapping = aes(x = date, y = hits, color = keyword)) +
  geom_line() + 
  geom_vline(xintercept = eventdate)
```

##### **Question 3**
Looking at your graph from problem 2, comment on (a) whether it looks like the lockdown had an effect on the popularity of sourdough, (b) the shape that effect takes (i.e. is it a permanent increase in popularity? Temporary?), (c) whether you might be concerned about any of the control groups we’ve chosen
**3a.** The lock down seems to be positively associated with sourdough’s popularity

**3b.** While there is an initial spike, the popularity gradually decreases through July dun dun dunnnnnn. It’s popularity in August is still higher than pre-lockdown. We can't really speak to how permanent/temporary these trended are without more data

**3c.** I meannnn logically, these controls may sort of suck. For example who compares cereal and sandwiches to sourdough? Specifically, those do not (likely) need to look up recipes for them when purchased. Also soupy-soup is seasonal! I love a good winter soup and ppl are less likely to eat soup in the summer (it is hot). Still, we see a spike near the onset of the pandemic (maybe like flu wanting soup or it was like v cold idk). But since the distance between the soup line and the sourdough line varied so much leading up to the start date, we shouldn’t use soup as a control. Sandwiches might also be more of a seasonal food, with consumption increasing in the warmer months. But our pre-treatmnet data does not extend far enough back to know.

##### **Question 4**
Create a “Treated” indicator that’s equal to 1 for sourdough and 0 otherwise (or True/False, either way). Do a test of whether the prior trends (keeping March 15 as the “treatment date”) differ between the treated and control groups, using a linear trend and doing a statistical significance test at the 95% level. Then, if you were concerned about any of the control groups in question 3c, drop any you were concerned about (and keep them dropped for the rest of the assignment) and rerun the test.
Write a line commenting on whether you can reject equal prior trends in your model(s).

```{r}
library(fixest)
library(modelsummary)

#making treated indicator
sr <- sr %>% 
  mutate(Treated = if_else(keyword == 'sourdough', 1L, 0L))

#party time new data set so we have the pre-treatment dates seperated so we can run the model
ptsr <- sr %>%
  filter(date < eventdate)
m1pt <- lm(hits ~ date + keyword + date*keyword,
           data = ptsr)
msummary(m1pt, stars = TRUE)

#now we are taking out the soup and seeing whats up
ptsr2 <- ptsr %>%
  filter(keyword != 'soup')

m2pt <- lm(hits ~ date + keyword + date*keyword,
           data = ptsr2)
msummary(m2pt, stars = TRUE)

#new data set again, estimates & seeing what's up without sandwhich
ptsr3 <- ptsr2  %>% 
  filter(keyword != 'sandwich')
m3pt <- lm(hits ~ date + keyword + date*keyword,
           data = ptsr3)
msummary(m3pt, stars = TRUE)

sr <- sr %>% 
  filter(keyword != "soup" & keyword != "sandwich")
```


##### **Question 5**
Create a month variable by shifting the date variable back 15 days (so that the treatment day is the first day of the month) and then taking the month of the resulting date. Also create an After variable equal to 1/0 (or True/False) if the date is March 15 or afterwards.
Then, take a look at the values of month you get and how they line up with date, and subtract a number from month so that the last period just before treatment (Feb 16-Mar 14) is 0. (Also, change the Jan 1-14 month so it’s one less than the Jan 15-Feb 14 month)
Then, use two-way fixed effects to estimate the difference-in-difference estimate of the effect of lockdown on sourdough popularity with keyword and month fixed effects, and standard errors clustered at the keyword level.

Creating `month` and `after` variable, then estimating model!
```{r}
#create month variable 
sr <- sr %>% 
  mutate(month = month(date - days(14)))
sr <- sr %>% 
  mutate(month = month-02)
sr <- sr %>% 
  mutate(month = if_else(month > 9, -2, month))

sr <- sr %>% 
  mutate(After = if_else(date >= eventdate, 1L, 0L ))


m5 <- feols(hits ~ After*Treated | keyword + month,
            data = sr)
msummary(m5, stars = TRUE)
```

##### **Question 6**
Now, let’s allow the effect to be dynamic over time. Estimate a difference-in-difference model allowing the effect to differ by month (using month = 0 as a reference period), with standard errors clustered at the keyword level, and show the results.

Okay, creating the sourdough variable, specifying the model & results
```{r}
sr <- sr %>% 
  mutate(sourdough = keyword == 'sourdough')

m6 <- feols(hits ~ i(month, sourdough, ref = 0) | keyword + month,
            data = sr)
msummary(m6, stars = TRUE)
```

##### **Question 7**
Make a graph demonstrating the results of your dynamic difference-in-differences model. Describe both what the effect looks like and also whether this graph gives you any concerns about prior trends violations.

Can do!
```{r}
coefplot(m6)
```
We see an effect around 0 in the pre-treatment groups & a big spike at the onset of the treatment period, with a peak in the following period and then a rapid decline in subsequent periods. Since the effects in the pre-treatment periods are both around 0, I am not worried about any violated trend assumptions.














